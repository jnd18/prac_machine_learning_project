---
title: "Practical Machine Learning Final Project"
author: "Jonathan Dorsey"
date: "June 11, 2018"
output: 
    html_document:
        keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(warn = -1) # use warn = 0 to turn back on
set.seed(1234)
```
## Outline

In this report we detail the model building process we used to achieve over 99.9% accuracy in classifying the manner in which an exericse was performed using information from accelerometers. The model used was a random forest.


## Background
From the course project instructions:

> Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 

In other words, we are given a data set of accelerometer measurements, and we must predict in which of five ways (A, B, C, D, or E) an exercise was performed.

## Preprocessing

First we download and load both the training set and the test set. The test set is set aside to the very end. Until then, when we mention "the data" we mean the training set.

```{r}
suppressPackageStartupMessages(library(tidyverse)) 
suppressPackageStartupMessages(library(caret)) 
suppressPackageStartupMessages(library(kableExtra)) 

train_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"


if (!file.exists("train.csv")) {
    download.file(train_url, "train.csv")
}
if (!file.exists("test.csv")) {
    download.file(test_url, "test.csv")
}

training <- read_csv("train.csv", col_types = cols())
testing <- read_csv("test.csv", col_types = cols())
```

Taking a quick look at the data, some columns appear to be almost entirely missing values. The table below shows the number of missing values (left column) and the number of columns that have that many missing values (right column).

```{r}
map(training, ~sum(is.na(.))) %>%
    as.numeric %>%
    table %>%
    as_data_frame %>%
    kable %>%
    kable_styling(full_width = F)
```

So we'll remove those columns which have 19,000 or more missing values. After that, there are just three columns each with one missing value, so we can just omit any row with a missing value, and we'll lose at most only three rows. Also, we'll remove the first column because it just gives the row number. Since the classes come in order, i.e. all A's first, then all the B's, etc., the row number completely messes up the prediction algorithm later. Specifically, since the test set's rows are numbered 1 to 20, the algorithm will predict that the class should always be A, since low row numbers act as a perfect predictor of class A in the training data. Thus, we're left with just 59 predictors of the original 160.

```{r}
training <- training %>% select_if(~sum(is.na(.)) < 19000) %>% na.omit
```

Finally, we'll split a validation set off from the training data, so that we can get an assessment of model accuracy later. We'll use 80% of the data for training and 20% for validation.

```{r}
indices <- createDataPartition(training$classe, p = 0.8, list = FALSE)
val <- training[-indices, ]
training <- training[indices, ]
```

## Model Fitting and Results

Now we will train a random forest using the `ranger` package through `caret`. In the interest of speed, we use the "out-of-bag" or OOB resampling option to tune the model hyperparameters. This means that the performance of the random forest is assessed by having it classify each training point, but using only those trees whose bootstrapped sample did not contain that point. This means the random forest only has to be fit once with each hyperparameter combination as opposed to cross-validation which would require refitting the entire model 10 times for each configuration. The OOB estimate is generally comparable to cross-validation, and this built-in cross-validation substitute is one of the nice features of random forests.

```{r, cache = TRUE}
model <- train(classe ~ .,
               data = as.data.frame(training),
               method = "ranger",
               trControl = trainControl(method = "oob"),
               na.action = na.omit)
model
```

The print-out above shows the final settings. The hyperparameters were `mtry = 41`, `splitrule = extratrees`, and `min.node.size = 1`. The most important hyperparameter is probably `mtry` which is the number of predictors to choose from at each split. See the documentation of the `ranger` package for a full explanation.

We can also see that the accuracy is extremely high, actually 100%. This is the out of bag error, not the training error. Because the model seemed to be so accurate, we felt no need to try another model. So we went straight to testing the model on the validation set. 

```{r}
confusionMatrix(predict(model, val), as.factor(val$classe))
```

The confusion matrix above shows the results on the validation data. The accuracy was actually 100%, with a 95% confidence interval of (0.9991, 1). Thus, we are 95% confident that the model's out of sample error is at least 99.9%. This is obviously a huge improvement over the no information rate (i.e. expected accuracy from random guessing) of 28%. Of course, this should be taken with a small grain of salt because, for example, the IID assumption used to form this estimate probably doesn't hold exactly. Regardless, the model did correctly predict all 20 test cases, according to the prediction quiz. We consider this a highly successful model.

## Appendix

Below is all the session info needed to make the report fully reproducible. Also, the seed was set to 1234, and warnings were suppressed. The full `.Rmd` file is available in the repository.

```{r}
sessionInfo()
```

